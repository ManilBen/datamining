{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa105b02-1f7c-4afb-b005-ff08b6849730",
   "metadata": {
    "id": "aa105b02-1f7c-4afb-b005-ff08b6849730"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "from csv import writer\n",
    "import re\n",
    "import nltk\n",
    "# Garbage collector library (to manage memory)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ab8f0-2dcc-4896-9043-8599df370a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell if you have not downloaded these packages yet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d53d5d-b74d-4fdb-84f8-57797f26b078",
   "metadata": {
    "id": "b0d53d5d-b74d-4fdb-84f8-57797f26b078"
   },
   "source": [
    "## Converting XML files into a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8de813-6a5a-4390-9d90-18677261b259",
   "metadata": {},
   "source": [
    "### Use the XML files to generate a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c5eaf-6fb2-4b28-9cfb-6103d51c9ae6",
   "metadata": {
    "id": "ca1c5eaf-6fb2-4b28-9cfb-6103d51c9ae6"
   },
   "outputs": [],
   "source": [
    "def export_to_csv(output_file):\n",
    "    \"\"\"Takes all the posts in the blogs directory, merges all the information and puts it in a CSV file.\"\"\"\n",
    "    post_regex = re.compile('<post>\\\\s*(.*)\\\\s*</post>')\n",
    "    # Selection of XML files\n",
    "    files = [x for x in os.listdir(\"blogs\") if x.endswith('.xml')]\n",
    "    # Current index to be printed\n",
    "    with open(output_file, 'w+') as outfile:\n",
    "        outfile.write(\"Id,Gender,Age,Industry,Zodiac,Date,Post\\n\")\n",
    "        writer_obj = writer(outfile)\n",
    "        for f in files:\n",
    "            # Get information from the file name\n",
    "            id, gender, age, industry, zodiac = f.split('.')[:5]\n",
    "            with open(os.path.join('blogs', f), 'r', encoding='utf-8', errors='ignore') as file_stream:\n",
    "                data = file_stream.read()\n",
    "                if not data:\n",
    "                    print('...')\n",
    "                # Extract the date and the content of the post\n",
    "                bs_data = BeautifulSoup(data, \"xml\")\n",
    "                list_dates = bs_data.find_all('date')\n",
    "                list_posts = bs_data.find_all('post')\n",
    "                for i, date_tag in enumerate(list_dates):\n",
    "                    tag_content = date_tag.contents[0]\n",
    "                    try:\n",
    "                        # Convert the date in yyyy-mm-dd format\n",
    "                        date_obj = dateparser.parse(tag_content, date_formats=['%d,%B,%Y'])\n",
    "                        if date_obj is not None:\n",
    "                            formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "                        else:\n",
    "                            formatted_date = None\n",
    "                        post = list_posts[i]\n",
    "                    except ValueError as e:\n",
    "                        formatted_date = None\n",
    "                        post = list_posts[i]\n",
    "                        print(f\"Value error: {e}\")\n",
    "                    if post is not None:\n",
    "                        # Delete the <post> and </post> tags\n",
    "                        m = re.match(post_regex, str(post))\n",
    "                        if m is not None:\n",
    "                            post = m.group(1)\n",
    "                    # Add a row in the file\n",
    "                    writer_obj.writerow([id, gender, age, industry, zodiac, formatted_date, post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a815ae-b2a8-47e2-b232-a244bc6d8f4e",
   "metadata": {
    "id": "a6a815ae-b2a8-47e2-b232-a244bc6d8f4e"
   },
   "outputs": [],
   "source": [
    "export_to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e782b0-3030-44ba-a8ce-1969f6ac32be",
   "metadata": {},
   "source": [
    "### Generate the n-grams and add them in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a267d-12c1-457d-88b1-9f88803c7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=2):\n",
    "    '''\n",
    "    Get the n-grams of a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text where n-grams are extracted\n",
    "        n (int): Number of elements of n-grams\n",
    "    Returns:\n",
    "        ngrams (list): List of n-grams\n",
    "    '''\n",
    "    text = str(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = nltk.ngrams(tokens, n, pad_left=\"_\", pad_right=\"_\")\n",
    "    del text\n",
    "    del tokens\n",
    "    gc.collect()\n",
    "    return list(ngrams)\n",
    "\n",
    "def add_ngrams(df):\n",
    "    '''\n",
    "    Adds the n-grams into the data frame. A column containing the n-grams\n",
    "    is added into the data frame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): data frame where n-grams are added\n",
    "    Returns:\n",
    "        df (DataFrame): data frame with the n-grams\n",
    "    '''\n",
    "    posts = df['Post'].values\n",
    "    df['Ngrams'] = [get_ngrams(p) for p in posts]\n",
    "    del posts\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def get_pos(text):\n",
    "    '''\n",
    "    Get the n-grams of a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Text where POS are extracted\n",
    "    Returns:\n",
    "        tags (list): List of tuples, each tuple containing a token and its POS\n",
    "    '''\n",
    "    text = str(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.tag.pos_tag(tokens, tagset='universal')\n",
    "    del text\n",
    "    del tokens\n",
    "    gc.collect()\n",
    "    return tags\n",
    "\n",
    "def add_pos(df):\n",
    "    '''\n",
    "    Adds the POS into the data frame. A column containing the POS\n",
    "    is added into the data frame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): data frame where POS are added\n",
    "    Returns:\n",
    "        df (DataFrame): data frame with POS\n",
    "    '''\n",
    "    posts = df['Post'].values\n",
    "    df['Ngrams'] = [get_ngrams(p) for p in posts]\n",
    "    del posts\n",
    "    gc.collect()\n",
    "    return df_copy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
